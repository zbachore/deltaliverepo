# Disable automatic pipeline triggers
trigger:
- none

# Use the latest Ubuntu image for the pipeline agents
pool:
  vmImage: 'ubuntu-latest'

# ---------------------------
# Stage 1: Deploy to Dev Workspace
# ---------------------------
stages:
- stage: DeployToDev
  displayName: 'Deploy Notebooks to Dev'
  variables:
  - group: Databricks-Dev

  jobs:
  - job: DeployDev
    steps:
    - checkout: self

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'

    - script: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | bash
        databricks --version
      displayName: 'Install Databricks CLI v2'

    - script: |
        echo "Deploying notebooks to Dev..."
        databricks workspace import-dir dlt-config-demo /Shared/DLT/dlt-config-demo --overwrite
        databricks workspace import-dir dlt-scd-demo /Shared/DLT/dlt-scd-demo --overwrite
      displayName: 'Deploy Notebooks to Dev Workspace'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

# ---------------------------
# Stage 2: Deploy to QA Workspace
# ---------------------------
- stage: DeployToQA
  displayName: 'Deploy Notebooks to QA'
  dependsOn: DeployToDev
  variables:
  - group: Databricks-QA

  jobs:
  - job: DeployQA
    steps:
    - checkout: self

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'

    - script: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | bash
        databricks --version
      displayName: 'Install Databricks CLI v2'

    - script: |
        echo "Deploying notebooks to QA..."
        databricks workspace import-dir dlt-config-demo /Shared/DLT/dlt-config-demo --overwrite
        databricks workspace import-dir dlt-scd-demo /Shared/DLT/dlt-scd-demo --overwrite
      displayName: 'Deploy Notebooks to QA Workspace'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

    # ----------------------------------------
    # ðŸ”§ Inject CATALOG_NAME into JSON in-place
    # ----------------------------------------
    - script: |
        echo "Replacing catalog_name in JSON file with value from variable group..."

        python3 -c "
        import json
        with open('dlt-config-demo/dlt-pipeline.json', 'r+') as f:
            data = json.load(f)
            data['catalog'] = '${CATALOG_NAME}'
            if 'configuration' in data:
                data['configuration']['catalog_name'] = '${CATALOG_NAME}'
            f.seek(0)
            json.dump(data, f, indent=2)
            f.truncate()
        "
      displayName: 'Update catalog_name in DLT JSON file'
      env:
        CATALOG_NAME: $(CATALOG_NAME)

    - script: |
        echo "Deploying DLT pipeline to QA..."

        PIPELINE_ID=$(databricks pipelines list --output json | jq -r '.[] | select(.name=="DLT-Pipeline-Configuration-Driven") | .pipeline_id')

        if [ -z "$PIPELINE_ID" ]; then
          echo "Pipeline not found. Creating it..."
          databricks pipelines create --json @dlt-config-demo/dlt-pipeline.json
        else
          echo "Pipeline exists. Updating it..."
          databricks pipelines update --pipeline-id "$PIPELINE_ID" --json-file dlt-config-demo/dlt-pipeline.json
        fi
      displayName: 'Deploy DLT Pipeline (Configuration-Driven)'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)
