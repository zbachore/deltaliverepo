# Disable automatic pipeline triggers
trigger:
- none

# Use the latest Ubuntu image for the pipeline agents
pool:
  vmImage: 'ubuntu-latest'

# ---------------------------
# Stage 1: Deploy to Dev Workspace
# ---------------------------
stages:
- stage: DeployToDev
  displayName: 'Deploy Notebooks to Dev'

  # Load variables (e.g., DATABRICKS_HOST, DATABRICKS_TOKEN) from the 'Databricks-Dev' variable group
  variables:
  - group: Databricks-Dev

  jobs:
  - job: DeployDev
    steps:
    
    # Checkout the GitHub repo
    - checkout: self

    # Use Python 3.x version for CLI tools
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'

    # Install Databricks CLI v2 and verify installation
    - script: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | bash
        databricks --version
      displayName: 'Install Databricks CLI v2'

    # Import notebooks into /Shared/DLT folder in the Dev Databricks workspace
    - script: |
        echo "Deploying notebooks to Dev..."
        databricks workspace import-dir dlt-config-demo /Shared/DLT/dlt-config-demo --overwrite
        databricks workspace import-dir dlt-scd-demo /Shared/DLT/dlt-scd-demo --overwrite
      displayName: 'Deploy Notebooks to Dev Workspace'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

# ---------------------------
# Stage 2: Deploy to QA Workspace
# ---------------------------
- stage: DeployToQA
  displayName: 'Deploy Notebooks to QA'
  dependsOn: DeployToDev

  # Load variables (e.g., DATABRICKS_HOST, DATABRICKS_TOKEN, CATALOG_NAME) from 'Databricks-QA' group
  variables:
  - group: Databricks-QA

  jobs:
  - job: DeployQA
    steps:

    # Checkout the GitHub repo
    - checkout: self

    # Use Python 3.x version for CLI tools
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'

    # Install Databricks CLI v2 and verify installation
    - script: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | bash
        databricks --version
      displayName: 'Install Databricks CLI v2'

    # Import notebooks into /Shared/DLT folder in the QA Databricks workspace
    - script: |
        echo "Deploying notebooks to QA..."
        databricks workspace import-dir dlt-config-demo /Shared/DLT/dlt-config-demo --overwrite
        databricks workspace import-dir dlt-scd-demo /Shared/DLT/dlt-scd-demo --overwrite
      displayName: 'Deploy Notebooks to QA Workspace'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

    # Replace devcatalog with actual CATALOG_NAME in pipeline JSON before deployment
    - script: |
        echo "Replacing catalog name in pipeline JSON..."
        sed -i "s/devcatalog/$(CATALOG_NAME)/g" dlt-config-demo/dlt-pipeline.json
      displayName: 'Update catalog name in DLT pipeline JSON'
      env:
        CATALOG_NAME: $(CATALOG_NAME)

    # Create or update the DLT pipeline in QA
    - script: |
        echo "Deploying DLT pipeline to QA..."

        PIPELINE_ID=$(databricks pipelines list --output json | jq -r '.[] | select(.name=="DLT-Pipeline-Configuration-Driven") | .pipeline_id')

        if [ -z "$PIPELINE_ID" ]; then
          echo "Pipeline not found. Creating it..."
          databricks pipelines create --json @dlt-config-demo/dlt-pipeline.json
        else
          echo "Pipeline exists. Updating it..."
          databricks pipelines update --pipeline-id "$PIPELINE_ID" --json-file dlt-config-demo/dlt-pipeline.json
        fi
      displayName: 'Deploy DLT Pipeline (Configuration-Driven)'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)
