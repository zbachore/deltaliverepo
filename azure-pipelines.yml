# Disable automatic pipeline triggers
trigger:
- none

# Use the latest Ubuntu image for the pipeline agents
pool:
  vmImage: 'ubuntu-latest'

# ---------------------------
# Stage 1: Deploy to Dev Workspace
# ---------------------------
stages:
- stage: DeployToDev
  displayName: 'Deploy Notebooks to Dev'

  variables:
  - group: Databricks-Dev

  jobs:
  - job: DeployDev
    steps:
    - checkout: self

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'

    - script: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | bash
        databricks --version
      displayName: 'Install Databricks CLI v2'

    - script: |
        echo "Deploying notebooks to Dev..."
        databricks workspace import-dir dlt-config-demo /Shared/DLT/dlt-config-demo --overwrite
        databricks workspace import-dir dlt-scd-demo /Shared/DLT/dlt-scd-demo --overwrite
      displayName: 'Deploy Notebooks to Dev Workspace'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

# ---------------------------
# Stage 2: Deploy to QA Workspace
# ---------------------------
- stage: DeployToQA
  displayName: 'Deploy Notebooks to QA'
  dependsOn: DeployToDev

  variables:
  - group: Databricks-QA

  jobs:
  - job: DeployQA
    steps:
    - checkout: self

    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'

    - script: |
        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | bash
        databricks --version
      displayName: 'Install Databricks CLI v2'

    # Replace "devcatalog" with value of CATALOG_NAME using Python
    - script: |
        echo "Replacing 'devcatalog' with '$CATALOG_NAME' using Python..."
        python3 -c "
        import json

        file_path = 'dlt-config-demo/dlt-pipeline.json'
        with open(file_path, 'r') as f:
            content = f.read()
        content = content.replace('devcatalog', '$CATALOG_NAME')
        with open(file_path, 'w') as f:
            f.write(content)
        "
      displayName: 'Update catalog name using Python'
      env:
        CATALOG_NAME: $(CATALOG_NAME)

    # Create or update the DLT pipeline in QA using the modified JSON
    - script: |
        echo "Deploying DLT pipeline to QA..."

        PIPELINE_ID=$(databricks pipelines list-pipelines --output json | jq -r '.[] | select(.name=="DLT-Pipeline-Configuration-Driven") | .pipeline_id')

        echo "Found PIPELINE_ID: $PIPELINE_ID"

        if [ -z "$PIPELINE_ID" ]; then
          echo "Pipeline not found. Creating it..."
          databricks pipelines create --json @dlt-config-demo/dlt-pipeline.json
        else
          echo "Pipeline exists. Updating it..."

          # Create a temp copy of JSON with injected pipeline_id
          python3 -c "
          import json
          input_path = 'dlt-config-demo/dlt-pipeline.json'
          output_path = 'dlt-config-demo/dlt-pipeline-updated.json'
          with open(input_path) as f:
              data = json.load(f)
          data['id'] = '$PIPELINE_ID'
          with open(output_path, 'w') as f:
              json.dump(data, f)
          "
          databricks pipelines update --pipeline-id "$PIPELINE_ID" --json @dlt-config-demo/dlt-pipeline-updated.json
        fi
      displayName: 'Create or Update DLT Pipeline in QA'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)


    - script: |
        echo "Deploying notebooks to QA..."
        databricks workspace import-dir dlt-config-demo /Shared/DLT/dlt-config-demo --overwrite
        databricks workspace import-dir dlt-scd-demo /Shared/DLT/dlt-scd-demo --overwrite
      displayName: 'Deploy Notebooks to QA Workspace'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)

    # Create or update the DLT pipeline in QA
    - script: |
        echo "Deploying DLT pipeline to QA..."

        PIPELINE_ID=$(databricks pipelines list --output json | jq -r '.[] | select(.name=="DLT-Pipeline-Configuration-Driven") | .pipeline_id')

        if [ -z "$PIPELINE_ID" ]; then
          echo "Pipeline not found. Creating it..."
          databricks pipelines create --json @dlt-config-demo/dlt-pipeline.json
        else
          echo "Pipeline exists. Updating it..."
          databricks pipelines update --pipeline-id "$PIPELINE_ID" --json-file dlt-config-demo/dlt-pipeline.json
        fi
      displayName: 'Deploy DLT Pipeline (Configuration-Driven)'
      env:
        DATABRICKS_HOST: $(DATABRICKS_HOST)
        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)
        CATALOG_NAME: $(CATALOG_NAME)
